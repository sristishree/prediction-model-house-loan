{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import logging\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, MinMaxScaler\n",
    "from sklearn.compose import make_column_transformer\n",
    "\n",
    "from sklearn.metrics import r2_score, accuracy_score, make_scorer\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression, Lasso, LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import re\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.DataFrame()\n",
    "predict_data = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LabelEncoderMultiColumns(data, columns_list):\n",
    "    \"\"\"Encoding categorical feature in the dataframe\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data: input dataframe \n",
    "    columns_list: categorical features list\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    data: new dataframe where categorical features are encoded\n",
    "    \"\"\"\n",
    "    labelencoder = LabelEncoder()\n",
    "    for col in columns_list:\n",
    "        data[col] = labelencoder.fit_transform(data[col])\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(y_true, y_pred, **kwargs):\n",
    "    return max(0, 100*r2_score(y_true, y_pred))\n",
    "r2 = make_scorer(score, greater_is_better = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KFoldCV(n_splits = 10):\n",
    "    # k-fold cross validation\n",
    "    kfold = KFold(n_splits=n_splits, shuffle=True)\n",
    "    return kfold\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LogisticRegressionModel(X, y):\n",
    "    logging.info('Started Grid Search for Logistic Regression...')\n",
    "    \n",
    "    # Add column for classification labels, to tell if the person is eligible for loan sanction or not\n",
    "    X['Loan Eligibility'] = np.where(y>0, 1, 0)\n",
    "    y = X['Loan Eligibility']\n",
    "    X.drop('Loan Eligibility', axis = 1, inplace = True)\n",
    "    \n",
    "    cv_outer = KFold(n_splits=10, shuffle=True, random_state=1)\n",
    "    # enumerate splits\n",
    "    outer_results = list()\n",
    "    for train_ix, test_ix in cv_outer.split(X):\n",
    "        # split data\n",
    "        X_train, X_test = X.iloc[train_ix], X.iloc[test_ix]\n",
    "        y_train, y_test = y.iloc[train_ix], y.iloc[test_ix]\n",
    "        # configure the cross-validation procedure\n",
    "        cv_inner = KFold(n_splits=3, shuffle=True, random_state=1)\n",
    "        # define the model\n",
    "        model = LogisticRegression(random_state=0, max_iter = 500)\n",
    "        # define search space\n",
    "        param_grid = [\n",
    "                {'penalty': ['l2'], 'solver': [ 'lbfgs', 'liblinear', 'sag', 'saga', 'newton-cg']},\n",
    "                {'penalty': ['l1'], 'solver': ['liblinear', 'saga']},\n",
    "        ]\n",
    "        # define search\n",
    "        search = GridSearchCV(model, param_grid = param_grid, scoring='accuracy', cv=cv_inner, refit=True)\n",
    "        # execute search\n",
    "        result = search.fit(X_train, y_train)\n",
    "        # get the best performing model fit on the whole training set\n",
    "        best_model = result.best_estimator_\n",
    "        # evaluate model on the hold out dataset\n",
    "        yhat = best_model.predict(X_test)\n",
    "        # evaluate the model\n",
    "        acc = accuracy_score(y_test, yhat)\n",
    "        # store the result\n",
    "        outer_results.append(acc)\n",
    "        # report progress\n",
    "        logging.info('acc = {}, est = {}, cfg = {}'.format(acc, result.best_score_, result.best_params_))\n",
    "    # summarize the estimated performance of the model\n",
    "    logging.info('\\nAccuracy: {} (Standard Deviation: {})'.format(np.mean(outer_results), np.std(outer_results)))\n",
    "    logging.info('Stopped Grid Search for Logistic Regression')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DecisionTreeModel(X, y):\n",
    "    logging.info('Started Grid Search for Decision Tree...')\n",
    "    \n",
    "    # Add column for classification labels, to tell if the person is eligible for loan sanction or not\n",
    "    X['Loan Eligibility'] = np.where(y>0, 1, 0)\n",
    "    y = X['Loan Eligibility']\n",
    "    X.drop('Loan Eligibility', axis = 1, inplace = True)\n",
    "    \n",
    "    cv_outer = KFold(n_splits=10, shuffle=True, random_state=1)\n",
    "    # enumerate splits\n",
    "    outer_results = list()\n",
    "    for train_ix, test_ix in cv_outer.split(X):\n",
    "        # split data\n",
    "        X_train, X_test = X.iloc[train_ix], X.iloc[test_ix]\n",
    "        y_train, y_test = y.iloc[train_ix], y.iloc[test_ix]\n",
    "        # configure the cross-validation procedure\n",
    "        cv_inner = KFold(n_splits=3, shuffle=True, random_state=1)\n",
    "        # define the model\n",
    "        model = DecisionTreeClassifier(random_state=0)\n",
    "        # define search space\n",
    "        param_grid = [\n",
    "                {'criterion': ['entropy', 'gini'], 'max_depth': np.arange(3, 15)}, \n",
    "                {'min_samples_leaf': np.arange(1,10)},\n",
    "        ]\n",
    "        # define search\n",
    "        search = GridSearchCV(model, param_grid = param_grid, scoring='accuracy', cv=cv_inner, refit=True)\n",
    "        # execute search\n",
    "        result = search.fit(X_train, y_train)\n",
    "        # get the best performing model fit on the whole training set\n",
    "        best_model = result.best_estimator_\n",
    "        # evaluate model on the hold out dataset\n",
    "        yhat = best_model.predict(X_test)\n",
    "        # evaluate the model\n",
    "        acc = accuracy_score(y_test, yhat)\n",
    "        # store the result\n",
    "        outer_results.append(acc)\n",
    "        # report progress\n",
    "        logging.info('acc = {}, \\n best score = {}, \\n best params = {}, \\n'.format(acc, result.best_score_, result.best_params_))\n",
    "    # summarize the estimated performance of the model\n",
    "    logging.info('\\nAccuracy: {} (Standard Deviation: {})'.format(np.mean(outer_results), np.std(outer_results)))\n",
    "    logging.info(best_model)\n",
    "    logging.info('Stopped Grid Search for Decision Tree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LinearRegressionModel(X, y):\n",
    "    logging.info('Started Grid Search for Linear Regression...')\n",
    "        \n",
    "    cv_outer = KFold(n_splits=10, shuffle=True, random_state=1)\n",
    "    # enumerate splits\n",
    "    outer_results = list()\n",
    "    for train_ix, test_ix in cv_outer.split(X):\n",
    "        # split data\n",
    "        X_train, X_test = X.iloc[train_ix], X.iloc[test_ix]\n",
    "        y_train, y_test = y.iloc[train_ix], y.iloc[test_ix]\n",
    "        # configure the cross-validation procedure\n",
    "        cv_inner = KFold(n_splits=3, shuffle=True, random_state=1)\n",
    "        # define the model\n",
    "        model = LinearRegression()\n",
    "        # define search space\n",
    "        space = dict()\n",
    "        space['fit_intercept'] = [True, False]\n",
    "        space['normalize'] = [True, False]\n",
    "       \n",
    "        # define search\n",
    "        search = GridSearchCV(model, space, scoring=r2, cv=cv_inner, refit=True)\n",
    "        # execute search\n",
    "        result = search.fit(X_train, y_train)\n",
    "        # get the best performing model fit on the whole training set\n",
    "        best_model = result.best_estimator_\n",
    "        # evaluate model on the hold out dataset\n",
    "        yhat = best_model.predict(X_test)\n",
    "        # evaluate the model\n",
    "        acc = score(y_test, yhat)\n",
    "        # store the result\n",
    "        outer_results.append(acc)\n",
    "        # report progress\n",
    "        logging.info('acc = {}, est = {}, cfg = {}'.format(acc, result.best_score_, result.best_params_))\n",
    "    # summarize the estimated performance of the model\n",
    "    logging.info('\\nAccuracy: {} (Standard Deviation: {})'.format(np.mean(outer_results), np.std(outer_results)))\n",
    "    logging.info('Stopped Grid Search for Linear Regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LassoRegressionModel(X, y):\n",
    "    logging.info('Started Grid Search for Lasso Regression...')\n",
    "        \n",
    "    cv_outer = KFold(n_splits=10, shuffle=True, random_state=1)\n",
    "    # enumerate splits\n",
    "    outer_results = list()\n",
    "    for train_ix, test_ix in cv_outer.split(X):\n",
    "        # split data\n",
    "        X_train, X_test = X.iloc[train_ix], X.iloc[test_ix]\n",
    "        y_train, y_test = y.iloc[train_ix], y.iloc[test_ix]\n",
    "        # configure the cross-validation procedure\n",
    "        cv_inner = KFold(n_splits=3, shuffle=True, random_state=1)\n",
    "        # define the model\n",
    "        model = Lasso(max_iter = 2000)\n",
    "        # define search space\n",
    "        space = dict()\n",
    "        space['alpha'] = np.logspace(1e-5, 1)\n",
    "        space['fit_intercept'] = [True, False]\n",
    "        space['normalize'] = [True, False]\n",
    "       \n",
    "        # define search\n",
    "        search = GridSearchCV(model, space, scoring=r2, cv=cv_inner, refit=True)\n",
    "        # execute search\n",
    "        result = search.fit(X_train, y_train)\n",
    "        # get the best performing model fit on the whole training set\n",
    "        best_model = result.best_estimator_\n",
    "        # evaluate model on the hold out dataset\n",
    "        yhat = best_model.predict(X_test)\n",
    "        # evaluate the model\n",
    "        acc = score(y_test, yhat)\n",
    "        # store the result\n",
    "        outer_results.append(acc)\n",
    "        # report progress\n",
    "        logging.info('acc = {}, est = {}, cfg = {}'.format(acc, result.best_score_, result.best_params_))\n",
    "    # summarize the estimated performance of the model\n",
    "    logging.info('\\nAccuracy: {} (Standard Deviation: {})'.format(np.mean(outer_results), np.std(outer_results)))\n",
    "    logging.info('Stopped Grid Search for Lasso Regression')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVRModel(X, Y):\n",
    "    logging.info('Started Grid Search for Support Vector Regression...')\n",
    "        \n",
    "    cv_outer = KFold(n_splits=10, shuffle=True, random_state=1)\n",
    "    # enumerate splits\n",
    "    outer_results = list()\n",
    "    for train_ix, test_ix in cv_outer.split(X):\n",
    "        # split data\n",
    "        X_train, X_test = X.iloc[train_ix], X.iloc[test_ix]\n",
    "        y_train, y_test = y.iloc[train_ix], y.iloc[test_ix]\n",
    "        # configure the cross-validation procedure\n",
    "        cv_inner = KFold(n_splits=3, shuffle=True, random_state=1)\n",
    "        # define the model\n",
    "        model = SVR()\n",
    "        # define search space\n",
    "        space = dict()\n",
    "        space['C']= [0.1, 1]\n",
    "        space['gamma'] = [1,0.1,0.001]\n",
    "        space['kernel'] = ['rbf', 'sigmoid']\n",
    "        \n",
    "        # define search\n",
    "        search = GridSearchCV(model, space, scoring=r2, cv=cv_inner, refit=True)\n",
    "        # execute search\n",
    "        result = search.fit(X_train, y_train)\n",
    "        # get the best performing model fit on the whole training set\n",
    "        best_model = result.best_estimator_\n",
    "        # evaluate model on the hold out dataset\n",
    "        yhat = best_model.predict(X_test)\n",
    "        # evaluate the model\n",
    "        acc = score(y_test, yhat)\n",
    "        # store the result\n",
    "        outer_results.append(acc)\n",
    "        # report progress\n",
    "        logging.info('acc = {}, est = {}, cfg = {}'.format(acc, result.best_score_, result.best_params_))\n",
    "    # summarize the estimated performance of the model\n",
    "    logging.info('\\nAccuracy: {} (Standard Deviation: {})'.format(np.mean(outer_results), np.std(outer_results)))\n",
    "    logging.info('Stopped Grid Search for Support Vector Regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RandomForestModel(X, Y):\n",
    "    logging.info('Started Grid Search for Random Forest Regression...')\n",
    "        \n",
    "    cv_outer = KFold(n_splits=10, shuffle=True, random_state=1)\n",
    "    # enumerate splits\n",
    "    outer_results = list()\n",
    "    for train_ix, test_ix in cv_outer.split(X):\n",
    "        # split data\n",
    "        X_train, X_test = X.iloc[train_ix], X.iloc[test_ix]\n",
    "        y_train, y_test = y.iloc[train_ix], y.iloc[test_ix]\n",
    "        # configure the cross-validation procedure\n",
    "        cv_inner = KFold(n_splits=3, shuffle=True, random_state=1)\n",
    "        # define the model\n",
    "        model = RandomForestRegressor(random_state = 0)\n",
    "        # define search space\n",
    "        space = dict()\n",
    "        space['n_estimators'] = [100,1000,10000]\n",
    "        space['max_features'] = [\"auto\", \"sqrt\", \"log2\"]\n",
    "        space['min_samples_split'] = [2,4,8]\n",
    "        space['bootstrap'] = [True, False]\n",
    "        \n",
    "        # define search\n",
    "        search = GridSearchCV(model, space, scoring=r2, cv=cv_inner, refit=True)\n",
    "        # execute search\n",
    "        result = search.fit(X_train, y_train)\n",
    "        # get the best performing model fit on the whole training set\n",
    "        best_model = result.best_estimator_\n",
    "        # evaluate model on the hold out dataset\n",
    "        yhat = best_model.predict(X_test)\n",
    "        # evaluate the model\n",
    "        acc = score(y_test, yhat)\n",
    "        # store the result\n",
    "        outer_results.append(acc)\n",
    "        # report progress\n",
    "        logging.info('acc = {}, est = {}, cfg = {}'.format(acc, result.best_score_, result.best_params_))\n",
    "    # summarize the estimated performance of the model\n",
    "    logging.info('\\nAccuracy: {} (Standard Deviation: {})'.format(np.mean(outer_results), np.std(outer_results)))\n",
    "    logging.info('Stopped Grid Search for Random Forest Regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(train_location, predict_location):\n",
    "    global train_data\n",
    "    train_data = pd.read_csv(train_location)\n",
    "    global predict_data \n",
    "    predict_data = pd.read_csv(predict_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_values(data):\n",
    "    data['Dependents'].fillna(data['Dependents'].value_counts().idxmax(), inplace=True)\n",
    "    # For numeric columns\n",
    "    data.fillna(data.mean(), inplace = True)\n",
    "    \n",
    "    # For string columns\n",
    "    nan_columns = data.isna().any()\n",
    "    nan_columns = nan_columns[nan_columns == True]\n",
    "    # Replace missing value with the most common value of that column\n",
    "    for column in nan_columns.iteritems():\n",
    "        data[column[0]].fillna(data[column[0]].value_counts().idxmax(), inplace=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_values(data, test):\n",
    "    # For columns having two categories\n",
    "    \n",
    "    data = LabelEncoderMultiColumns(data, ['Gender', 'Income Stability', 'Expense Type 1', 'Expense Type 2'])\n",
    "\n",
    "    # For columns having multiple categories\n",
    "    if test==True:\n",
    "        data = onehotencoder.transform(data)\n",
    "    else:\n",
    "        onehotencoder.fit(data)\n",
    "        data = onehotencoder.transform(data)\n",
    "    column_names = onehotencoder.get_feature_names()\n",
    "    # Remove 'onehotencoder' from column labels\n",
    "    for index, name in enumerate(column_names):\n",
    "        column_names[index] = re.sub(r'onehotencoder__x', '', name)\n",
    "    data = pd.DataFrame(data, columns=column_names)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization(data, test):   \n",
    "    if test==True:\n",
    "        data = pd.DataFrame(minmaxscaler.transform(data), columns = data.columns)\n",
    "    else:\n",
    "        minmaxscaler.fit(data)\n",
    "        data = pd.DataFrame(minmaxscaler.transform(data), columns = data.columns)\n",
    "    return data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_train_data():\n",
    "    global train_data\n",
    "    # 1. Remove columns containing unimportant information\n",
    "    train_data.drop(['Customer ID', 'Name', 'Property ID'], axis = 1, inplace = True)\n",
    "    # 2. Replace any datapoints containing unwanted characters with NaN\n",
    "    train_data.replace({r'[+=!~`:;?<>@#$%^&*]': None}, regex = True, inplace = True)\n",
    "    # 3. Treat missing values\n",
    "    train_data = missing_values(train_data)\n",
    "    # 4. Seperate X_train and y_train for further pre-processing\n",
    "    train_Y = train_data['Loan Sanction Amount (USD)']\n",
    "    train_data = train_data.drop(['Loan Sanction Amount (USD)'], axis = 1)\n",
    "    # 5. Deal with columns containing categorical values - Dummy variables\n",
    "    global onehotencoder \n",
    "    onehotencoder = make_column_transformer((OneHotEncoder(), ['Location', 'Has Active Credit Card', 'Property Location', 'Profession', 'Type of Employment']), remainder='passthrough')\n",
    "\n",
    "    train_data = categorical_values(train_data, False)\n",
    "    # 6. Normalization\n",
    "    global minmaxscaler\n",
    "    minmaxscaler = MinMaxScaler()\n",
    "    train_data = normalization(train_data, False)\n",
    "    train_data['Loan Sanction Amount (USD)'] = train_Y\n",
    "    del train_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_predict_data():\n",
    "    global predict_data\n",
    "    global predict_index\n",
    "    # 1. Remove columns containing unimportant information\n",
    "    predict_index = predict_data['Customer ID']\n",
    "    predict_data.drop(['Customer ID', 'Name', 'Property ID'], axis = 1, inplace = True)\n",
    "    # 2. Replace any datapoints containing unwanted characters with NaN\n",
    "    predict_data.replace({r'[+=!~`:;?<>@#$%^&*]': None}, regex = True, inplace = True)\n",
    "    # 3. Treat missing values\n",
    "    predict_data = missing_values(predict_data)\n",
    "    # 4. Deal with columns containing categorical values - Dummy variables\n",
    "    predict_data = categorical_values(predict_data, True)\n",
    "    # 6. Normalization\n",
    "    predict_data = normalization(predict_data, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-28 10:18:32,005 :   Customer ID               Name Gender  Age  Income (USD) Income Stability  \\\n",
      "0     C-36995   Frederica Shealy      F   56       1933.05              Low   \n",
      "1     C-33999  America Calderone      M   32       4952.91              Low   \n",
      "2      C-3770      Rosetta Verne      F   65        988.19             High   \n",
      "3     C-26480         Zoe Chitty      F   65           NaN             High   \n",
      "4     C-23459       Afton Venema      F   31       2614.77              Low   \n",
      "\n",
      "  Profession     Type of Employment    Location  Loan Amount Request (USD)  \\\n",
      "0    Working            Sales staff  Semi-Urban                   72809.58   \n",
      "1    Working                    NaN  Semi-Urban                   46837.47   \n",
      "2  Pensioner                    NaN  Semi-Urban                   45593.04   \n",
      "3  Pensioner                    NaN       Rural                   80057.92   \n",
      "4    Working  High skill tech staff  Semi-Urban                  113858.89   \n",
      "\n",
      "   ...  Credit Score No. of Defaults Has Active Credit Card  Property ID  \\\n",
      "0  ...        809.44               0                    NaN          746   \n",
      "1  ...        780.40               0            Unpossessed          608   \n",
      "2  ...        833.15               0            Unpossessed          546   \n",
      "3  ...        832.70               1            Unpossessed          890   \n",
      "4  ...        745.55               1                 Active          715   \n",
      "\n",
      "   Property Age  Property Type Property Location  Co-Applicant  \\\n",
      "0       1933.05              4             Rural             1   \n",
      "1       4952.91              2             Rural             1   \n",
      "2        988.19              2             Urban             0   \n",
      "3           NaN              2        Semi-Urban             1   \n",
      "4       2614.77              4        Semi-Urban             1   \n",
      "\n",
      "   Property Price  Loan Sanction Amount (USD)  \n",
      "0       119933.46                    54607.18  \n",
      "1        54791.00                    37469.98  \n",
      "2        72440.58                    36474.43  \n",
      "3       121441.51                    56040.54  \n",
      "4       208567.91                    74008.28  \n",
      "\n",
      "[5 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "read_data('train.csv', 'test.csv')\n",
    "logging.info(train_data.head())\n",
    "clean_train_data()\n",
    "y = train_data['Loan Sanction Amount (USD)']\n",
    "X = train_data.drop(['Loan Sanction Amount (USD)'], axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-28 01:04:33,888 : Started Grid Search for Logistic Regression...\n",
      "2021-06-28 01:04:57,039 : acc = 0.823, est = 0.8201111111111111, cfg = {'penalty': 'l1', 'solver': 'liblinear'}\n",
      "2021-06-28 01:05:19,556 : acc = 0.821, est = 0.8194444444444443, cfg = {'penalty': 'l1', 'solver': 'liblinear'}\n",
      "2021-06-28 01:05:41,833 : acc = 0.8126666666666666, est = 0.8207407407407407, cfg = {'penalty': 'l1', 'solver': 'liblinear'}\n",
      "2021-06-28 01:06:03,512 : acc = 0.8156666666666667, est = 0.8205555555555556, cfg = {'penalty': 'l1', 'solver': 'saga'}\n",
      "2021-06-28 01:06:24,881 : acc = 0.8243333333333334, est = 0.8196296296296296, cfg = {'penalty': 'l1', 'solver': 'liblinear'}\n",
      "2021-06-28 01:06:44,794 : acc = 0.8193333333333334, est = 0.8191111111111112, cfg = {'penalty': 'l2', 'solver': 'sag'}\n",
      "2021-06-28 01:07:03,465 : acc = 0.8233333333333334, est = 0.8200740740740741, cfg = {'penalty': 'l2', 'solver': 'liblinear'}\n",
      "2021-06-28 01:07:20,945 : acc = 0.8146666666666667, est = 0.8221851851851852, cfg = {'penalty': 'l1', 'solver': 'saga'}\n",
      "2021-06-28 01:07:39,222 : acc = 0.824, est = 0.8194814814814815, cfg = {'penalty': 'l1', 'solver': 'saga'}\n",
      "2021-06-28 01:08:01,500 : acc = 0.8273333333333334, est = 0.8178148148148149, cfg = {'penalty': 'l1', 'solver': 'liblinear'}\n",
      "2021-06-28 01:08:01,508 : \n",
      "Accuracy: 0.8205333333333333 (Standard Deviation: 0.004563624290699964)\n",
      "2021-06-28 01:08:01,510 : Stopped Grid Search for Logistic Regression\n",
      "2021-06-28 01:08:01,513 : Started Grid Search for Decision Tree...\n",
      "2021-06-28 01:08:18,455 : acc = 0.903, \n",
      " best score = 0.9085185185185184, \n",
      " best params = {'criterion': 'entropy', 'max_depth': 5}, \n",
      "\n",
      "2021-06-28 01:08:35,301 : acc = 0.9116666666666666, \n",
      " best score = 0.9078148148148149, \n",
      " best params = {'criterion': 'entropy', 'max_depth': 5}, \n",
      "\n",
      "2021-06-28 01:08:51,850 : acc = 0.903, \n",
      " best score = 0.9087777777777778, \n",
      " best params = {'criterion': 'entropy', 'max_depth': 5}, \n",
      "\n",
      "2021-06-28 01:09:09,036 : acc = 0.9066666666666666, \n",
      " best score = 0.9080370370370371, \n",
      " best params = {'criterion': 'entropy', 'max_depth': 5}, \n",
      "\n",
      "2021-06-28 01:09:26,272 : acc = 0.9133333333333333, \n",
      " best score = 0.9073333333333333, \n",
      " best params = {'criterion': 'gini', 'max_depth': 5}, \n",
      "\n",
      "2021-06-28 01:09:43,577 : acc = 0.9073333333333333, \n",
      " best score = 0.9081111111111112, \n",
      " best params = {'criterion': 'entropy', 'max_depth': 5}, \n",
      "\n",
      "2021-06-28 01:10:00,851 : acc = 0.9086666666666666, \n",
      " best score = 0.9076666666666666, \n",
      " best params = {'criterion': 'entropy', 'max_depth': 5}, \n",
      "\n",
      "2021-06-28 01:10:18,662 : acc = 0.909, \n",
      " best score = 0.9080370370370371, \n",
      " best params = {'criterion': 'entropy', 'max_depth': 5}, \n",
      "\n",
      "2021-06-28 01:10:36,092 : acc = 0.9133333333333333, \n",
      " best score = 0.9073703703703705, \n",
      " best params = {'criterion': 'gini', 'max_depth': 5}, \n",
      "\n",
      "2021-06-28 01:10:53,953 : acc = 0.905, \n",
      " best score = 0.9085185185185186, \n",
      " best params = {'criterion': 'entropy', 'max_depth': 5}, \n",
      "\n",
      "2021-06-28 01:10:53,954 : \n",
      "Accuracy: 0.9080999999999999 (Standard Deviation: 0.0036455452267116248)\n",
      "2021-06-28 01:10:53,955 : DecisionTreeClassifier(criterion='entropy', max_depth=5, random_state=0)\n",
      "2021-06-28 01:10:53,976 : Stopped Grid Search for Decision Tree\n"
     ]
    }
   ],
   "source": [
    "LogisticRegressionModel(X, y)\n",
    "DecisionTreeModel(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_copy = train_data.copy()\n",
    "train_data_copy.drop(train_data_copy[train_data_copy['Loan Sanction Amount (USD)'] == 0].index, inplace = True)\n",
    "y_copy = train_data_copy['Loan Sanction Amount (USD)']\n",
    "X_copy = train_data_copy.drop(['Loan Sanction Amount (USD)'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-28 01:10:53,999 : Started Grid Search for Linear Regression...\n",
      "2021-06-28 01:10:54,794 : acc = 90.83977995026393, est = 0.0, cfg = {'fit_intercept': True, 'normalize': True}\n",
      "2021-06-28 01:10:55,383 : acc = 93.09239330284854, est = -61.0973963357464, cfg = {'fit_intercept': True, 'normalize': True}\n",
      "2021-06-28 01:10:55,925 : acc = 91.71756457204714, est = -30.780047434266805, cfg = {'fit_intercept': False, 'normalize': True}\n",
      "2021-06-28 01:10:56,461 : acc = 91.57824465917909, est = -30.536068265330226, cfg = {'fit_intercept': True, 'normalize': False}\n",
      "2021-06-28 01:10:57,057 : acc = 94.28404432867194, est = -30.329477419006214, cfg = {'fit_intercept': True, 'normalize': False}\n",
      "2021-06-28 01:10:57,563 : acc = 0, est = -61.71757643657153, cfg = {'fit_intercept': True, 'normalize': True}\n",
      "2021-06-28 01:10:58,330 : acc = 89.982385849823, est = -61.29881161018286, cfg = {'fit_intercept': True, 'normalize': False}\n",
      "2021-06-28 01:10:58,896 : acc = 93.88084158517997, est = -30.960757478631532, cfg = {'fit_intercept': True, 'normalize': False}\n",
      "2021-06-28 01:10:59,451 : acc = 0, est = -61.692887365223704, cfg = {'fit_intercept': False, 'normalize': True}\n",
      "2021-06-28 01:11:00,071 : acc = 93.26678025846536, est = -30.698652028897428, cfg = {'fit_intercept': True, 'normalize': False}\n",
      "2021-06-28 01:11:00,072 : \n",
      "Accuracy: 73.86420345064789 (Standard Deviation: 36.95423634562293)\n",
      "2021-06-28 01:11:00,073 : Stopped Grid Search for Linear Regression\n",
      "2021-06-28 01:11:00,074 : Started Grid Search for Lasso Regression...\n",
      "2021-06-28 01:16:45,436 : acc = 90.78609506661363, est = -92.24414048071627, cfg = {'alpha': 10.0, 'fit_intercept': True, 'normalize': True}\n",
      "2021-06-28 01:22:55,418 : acc = 92.90233798898596, est = -92.02272995899028, cfg = {'alpha': 10.0, 'fit_intercept': True, 'normalize': True}\n",
      "2021-06-28 01:28:29,298 : acc = 91.62899296522406, est = -92.17442619200921, cfg = {'alpha': 10.0, 'fit_intercept': True, 'normalize': True}\n",
      "2021-06-28 01:33:38,165 : acc = 91.36602760304996, est = -92.1758230833376, cfg = {'alpha': 10.0, 'fit_intercept': True, 'normalize': True}\n",
      "2021-06-28 01:39:34,275 : acc = 94.12361362191662, est = -91.85556097539607, cfg = {'alpha': 10.0, 'fit_intercept': True, 'normalize': True}\n",
      "2021-06-28 01:45:00,457 : acc = 91.26259708157193, est = -92.21335590123465, cfg = {'alpha': 10.0, 'fit_intercept': True, 'normalize': True}\n",
      "2021-06-28 01:51:45,570 : acc = 89.93787985444537, est = -92.34490027944035, cfg = {'alpha': 10.0, 'fit_intercept': True, 'normalize': True}\n",
      "2021-06-28 01:57:36,344 : acc = 93.659431947646, est = -91.92233421454677, cfg = {'alpha': 10.0, 'fit_intercept': True, 'normalize': True}\n",
      "2021-06-28 02:02:27,906 : acc = 91.57834840105036, est = -92.15394920994278, cfg = {'alpha': 10.0, 'fit_intercept': True, 'normalize': True}\n",
      "2021-06-28 02:08:10,257 : acc = 93.11880808979708, est = -92.01255639752235, cfg = {'alpha': 10.0, 'fit_intercept': True, 'normalize': True}\n",
      "2021-06-28 02:08:10,274 : \n",
      "Accuracy: 92.0364132620301 (Standard Deviation: 1.2768382972270527)\n",
      "2021-06-28 02:08:10,278 : Stopped Grid Search for Lasso Regression\n",
      "2021-06-28 02:08:10,281 : Started Grid Search for Support Vector Regression...\n",
      "2021-06-28 02:35:49,563 : acc = 0, est = 0.0, cfg = {'C': 0.1, 'gamma': 1, 'kernel': 'rbf'}\n",
      "2021-06-28 03:03:32,159 : acc = 0, est = 0.0, cfg = {'C': 0.1, 'gamma': 1, 'kernel': 'rbf'}\n",
      "2021-06-28 03:31:07,907 : acc = 0, est = 0.0, cfg = {'C': 0.1, 'gamma': 1, 'kernel': 'rbf'}\n",
      "2021-06-28 03:58:43,766 : acc = 0, est = 0.0, cfg = {'C': 0.1, 'gamma': 1, 'kernel': 'rbf'}\n",
      "2021-06-28 04:26:20,266 : acc = 0, est = 0.0, cfg = {'C': 0.1, 'gamma': 1, 'kernel': 'rbf'}\n",
      "2021-06-28 04:53:59,703 : acc = 0, est = 0.0, cfg = {'C': 0.1, 'gamma': 1, 'kernel': 'rbf'}\n",
      "2021-06-28 05:21:35,165 : acc = 0, est = 0.0, cfg = {'C': 0.1, 'gamma': 1, 'kernel': 'rbf'}\n",
      "2021-06-28 05:49:12,878 : acc = 0, est = 0.0, cfg = {'C': 0.1, 'gamma': 1, 'kernel': 'rbf'}\n",
      "2021-06-28 06:16:49,661 : acc = 0, est = 0.0, cfg = {'C': 0.1, 'gamma': 1, 'kernel': 'rbf'}\n",
      "2021-06-28 06:44:29,371 : acc = 0, est = 0.0, cfg = {'C': 0.1, 'gamma': 1, 'kernel': 'rbf'}\n",
      "2021-06-28 06:44:29,371 : \n",
      "Accuracy: 0.0 (Standard Deviation: 0.0)\n",
      "2021-06-28 06:44:29,372 : Stopped Grid Search for Support Vector Regression\n",
      "2021-06-28 06:44:29,373 : Started Grid Search for Random Forest Regression...\n"
     ]
    }
   ],
   "source": [
    "LinearRegressionModel(X_copy, y_copy)\n",
    "LassoRegressionModel(X_copy, y_copy)\n",
    "SVRModel(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_model(X, y):\n",
    "    # Model 1 - Classify whether Loan eligibility is True or False\n",
    "    \n",
    "    train_X, test_X, train_Y, test_Y = train_test_split(X, y, test_size=0.20, random_state=1)\n",
    "    logging.info('Started training the aggregate model...')\n",
    "    # Add column for classification labels, to tell if the person is eligible for loan sanction or not\n",
    "    X_copy = train_X.copy()\n",
    "    X_copy['Loan Eligibility'] = np.where(train_Y>0, 1, 0)\n",
    "    y_copy = X_copy['Loan Eligibility']\n",
    "    X_copy.drop('Loan Eligibility', axis = 1, inplace = True)\n",
    "    \n",
    "    X_test_copy = test_X.copy()\n",
    "    X_test_copy['Loan Eligibility'] = np.where(test_Y>0, 1, 0)\n",
    "    y_test_copy = X_test_copy['Loan Eligibility']\n",
    "    X_test_copy.drop('Loan Eligibility', axis = 1, inplace = True)\n",
    "    \n",
    "    global classification_model\n",
    "    classification_model = DecisionTreeClassifier(random_state=0, criterion = 'entropy', max_depth = 5)\n",
    "    classification_model.fit(X_copy, y_copy)\n",
    "    yhat_classification = classification_model.predict(X_test_copy)\n",
    "    yhat_classification = pd.DataFrame(yhat_classification, index = X_test_copy.index)\n",
    "    yhat_classification.drop(yhat_classification[yhat_classification[0] == 1].index, inplace = True)\n",
    "    #logging.info('Predict: {} Shape: {}'.format(prediction.head(), prediction.shape))\n",
    "    \n",
    "    # Model 2 - If Loan eligibility is True, predict the loan sanction amount\n",
    "    \n",
    "    # Keep only those data points where Loan Sanction Amount is greater that 0\n",
    "    X_copy = train_X.copy()\n",
    "    y_copy = train_Y.copy()\n",
    "    X_copy.drop(X_copy[train_Y == 0].index, inplace = True)\n",
    "    y_copy.drop(y_copy[train_Y == 0].index, inplace = True)\n",
    "    \n",
    "    X_test_copy = test_X.copy()\n",
    "    y_test_copy = test_Y.copy()\n",
    "    X_test_copy.drop(yhat_classification[yhat_classification[0] == 0].index, inplace = True)\n",
    "    y_test_copy.drop(yhat_classification[yhat_classification[0] == 0].index, inplace = True)\n",
    "    global regression_model\n",
    "    regression_model = LinearRegression()\n",
    "    regression_model.fit(X_copy, y_copy)\n",
    "    yhat_regression = regression_model.predict(X_test_copy)\n",
    "    yhat_regression = pd.DataFrame(yhat_regression, index = X_test_copy.index)\n",
    "    prediction = pd.concat([yhat_classification, yhat_regression])\n",
    "    prediction = prediction.sort_index(ascending=True)\n",
    "    logging.info(prediction.head())\n",
    "    test_Y = test_Y.sort_index(ascending=True)\n",
    "    scores = score(test_Y, prediction[0])\n",
    "    logging.info('Finished training the aggregate model.')\n",
    "    logging.info('Score: {}'.format(scores))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-28 10:12:50,535 : Started training the aggregate model...\n",
      "2021-06-28 10:12:50,722 :           0\n",
      "2   36528.0\n",
      "11      0.0\n",
      "19  78096.0\n",
      "22  11552.0\n",
      "23  82256.0\n",
      "2021-06-28 10:12:50,727 : Finished training the aggregate model.\n",
      "2021-06-28 10:12:50,728 : Score: 75.88097445433519\n"
     ]
    }
   ],
   "source": [
    "aggregate_model(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_predict_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_loan_sanction_amount():\n",
    "    predict_dat = predict_data.copy()\n",
    "    logging.info('Predicting based on the aggregate model...')\n",
    "    \n",
    "    yhat_classification = classification_model.predict(predict_dat)\n",
    "    yhat_classification = pd.DataFrame(yhat_classification, index = predict_dat.index)\n",
    "    yhat_classification.drop(yhat_classification[yhat_classification[0] == 1].index, inplace = True)\n",
    "    #logging.info('Predict: {} Shape: {}'.format(prediction.head(), prediction.shape))\n",
    "    \n",
    "    # Model 2 - If Loan eligibility is True, predict the loan sanction amount\n",
    "    \n",
    "    # Keep only those data points where Loan Sanction Amount is greater that 0\n",
    "    \n",
    "    predict_dat.drop(yhat_classification[yhat_classification[0] == 0].index, inplace = True)\n",
    "    yhat_regression = regression_model.predict(predict_dat)\n",
    "    yhat_regression = pd.DataFrame(yhat_regression, index = predict_dat.index)\n",
    "    prediction = pd.concat([yhat_classification, yhat_regression])\n",
    "    prediction = prediction.sort_index(ascending=True)\n",
    "    prediction['Customer ID'] = predict_index\n",
    "    prediction = prediction.rename(columns = {0: 'Loan sanction Amount (USD)'})\n",
    "    prediction = prediction[['Customer ID', 'Loan sanction Amount (USD)']]\n",
    "    logging.info('Shape of prediction file: {}'.format(prediction.shape))\n",
    "    logging.info('Saving to csv file...')\n",
    "    prediction.to_csv('prediction.csv', index = False)\n",
    "    logging.info('Saved')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-28 10:26:32,330 : Predicting based on the aggregate model...\n",
      "2021-06-28 10:26:32,370 : Shape of prediction file: (20000, 2)\n",
      "2021-06-28 10:26:32,371 : Saving to csv file...\n",
      "2021-06-28 10:26:32,683 : Saved\n"
     ]
    }
   ],
   "source": [
    "predict_loan_sanction_amount()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
